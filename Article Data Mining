import requests
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

[nltk_data] Downloading package punkt to
[nltk_data]     /Users/gannapogrebna/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Out[1]:
True
In [2]:
# Define the URL of the paper web page
url = 'https://www.frontiersin.org/articles/10.3389/fevo.2019.00006/full'
In [3]:
# Send a GET request to the web page
response = requests.get(url)
In [4]:
# Parse the content of the request with BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')
In [5]:
# Get the text from the BeautifulSoup object
text = soup.get_text()
In [6]:
# Tokenize the text into sentences
sentences = sent_tokenize(text)
In [7]:
# Define the topics you are interested in
topics = [
    “Topic1”,
    “Topic2”,
    “Topic3”,
    “Topic4”
]
In [12]:
import csv
import re
In [13]:
# Open a CSV file for writing: replace “output.csv” with the full path to the output file
with open('output.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Topic", "Sentence"])

    # For each topic
    for topic in topics:
        # Split the text into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)

        # For each sentence
        for sentence in sentences:
            # If the topic is in the sentence
            if topic in sentence:
                # Write the topic and sentence to the CSV file
                writer.writerow([topic, sentence])

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn import tree
import seaborn as sns
In [2]:
# Load the data - insert the path to the data file
df = pd.read_csv('rfout.csv')
In [3]:
print(df.columns)

Index(['DVSuccess', 'Nectar', 'Pollen', 'Both', 'Parasitoid', 'Predator',
       'Parasit&Pred', 'Beetles', 'Hoverflies', 'Lacewings', 'Mites', 'Wasps',
       '1_Field_Crops', '1_FTreesOrchards', '1_VegHerbs', '1_VineBerries',
       '1_SporMisCrops', '2_Cover_or_Cultivated_Crops', '2_Flowering_Plants',
       '2_Fruit_Trees', '2_Pasture_Forage', '2_Wild_Plants',
       '2_Combo_or_Miscellaneous'],
      dtype='object')
In [ ]:
import numpy as np
import seaborn as sns
from scipy.stats import pearsonr
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
correlation_matrix = df.corr()

# Create a DataFrame to hold the p-values
p_values = pd.DataFrame(index=df.columns, columns=df.columns)

# Calculate the p-values
for i in df.columns:
    for j in df.columns:
        p_values.loc[i, j] = round(pearsonr(df[i], df[j])[1], 4)

# Create a mask to display only the lower triangle of the matrix (since it's mirrored around its 
# top-left to bottom-right diagonal)
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Create a custom diverging palette
cmap = sns.diverging_palette(230, 20, as_cmap=True)

plt.figure(figsize=(18, 18))

# Draw the heatmap
sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0, square=True, 
            linewidths=.5, cbar_kws={"shrink": .5}, annot=p_values, fmt=".2f")

plt.title('Correlation Matrix with p-values', fontsize=20)
plt.show()
In [5]:
# Define the features and the target
features = ['Nectar', 'Pollen', 'Both', 'Beetles', 'Hoverflies', 'Lacewings',
       'Mites', 'Wasps', 'Field Crops', 'FTreesOrchards', 'VegHerbs',
       'VineBerries', 'SporMisCrops', '2ndField Crops', '2ndFTreesOrchards',
       '2ndVegHerbs', '2ndVineBerries', '2ndSporMisCrops', 'Parasitoid',
       'Predator', 'Parasit&Pred', 'EcosServices', 'FoodFieldCrops',
       'FlowePlants', 'FruitTrees', 'Grasses', 'Herbs', 'MixedFlowPl',
       'MixedPlants', 'WildFlowPlants']
target = 'DVSuccess'
In [6]:
X = df.drop('DVSuccess', axis=1)
y = df['DVSuccess']
In [7]:
import matplotlib.pyplot as plt
plt.boxplot(df['DVSuccess'])
plt.show()

￼
In [8]:
import matplotlib.pyplot as plt

plt.hist(df['DVSuccess'], bins=30, edgecolor='black')
plt.title('Histogram of Target Variable')
plt.xlabel('Target')
plt.ylabel('Frequency')
plt.show()

￼
In [9]:
Q1 = df['DVSuccess'].quantile(0.25)
Q3 = df['DVSuccess'].quantile(0.75)
IQR = Q3 - Q1

outliers = df['DVSuccess'][((df['DVSuccess'] < (Q1 - 1.5 * IQR)) | (df['DVSuccess'] > (Q3 + 1.5 * IQR)))]
In [10]:
# Print the outliers
print(outliers)

146    100
Name: DVSuccess, dtype: int64
In [11]:
# Calculate Q1, Q3, and IQR
Q1 = df['DVSuccess'].quantile(0.25)
Q3 = df['DVSuccess'].quantile(0.75)
IQR = Q3 - Q1

# Define the bounds for non-outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove the outliers
df_no_outliers = df[(df['DVSuccess'] >= lower_bound) & (df['DVSuccess'] <= upper_bound)]
In [13]:
features = ['Nectar', 'Pollen', 'Both', 'Parasitoid', 'Predator',
       'Parasit&Pred', 'Beetles', 'Hoverflies', 'Lacewings', 'Mites', 'Wasps',
       '1_Field_Crops', '1_FTreesOrchards', '1_VegHerbs', '1_VineBerries',
       '1_SporMisCrops', '2_Cover_or_Cultivated_Crops', '2_Flowering_Plants',
       '2_Fruit_Trees', '2_Pasture_Forage', '2_Wild_Plants',
       '2_Combo_or_Miscellaneous']
target = 'DVSuccess'

X_train, X_test, y_train, y_test = train_test_split(df_no_outliers[features], df_no_outliers[target], test_size=0.2, random_state=42)
In [14]:
# Initialize the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
In [15]:
# Fit the model
try:
    model.fit(X_train, y_train)
    print("Model trained successfully.")
except Exception as e:
    print("Error during model training: ", e)

Model trained successfully.
In [16]:
import numpy as np

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

# Take the square root of each score to get RMSE, then take the mean of those to get the average RMSE
avg_rmse = np.mean(np.sqrt(np.abs(scores)))

print('Average RMSE from 5-fold cross-validation:', avg_rmse)

Average RMSE from 5-fold cross-validation: 18.707547449551
In [17]:
# Get feature importances
try:
    importances = model.feature_importances_
    print("Feature importances calculated successfully.")
except Exception as e:
    print("Error during feature importance calculation: ", e)

Feature importances calculated successfully.
In [18]:
# Convert the importances into a DataFrame
try:
    importances_df = pd.DataFrame({'feature':features, 'importance':importances})
    print("Feature importances DataFrame created successfully.")
except Exception as e:
    print("Error during DataFrame creation: ", e)

Feature importances DataFrame created successfully.
In [19]:
# Sort the DataFrame by importance
try:
    importances_df = importances_df.sort_values('importance', ascending=False)
    print("DataFrame sorted successfully.")
except Exception as e:
    print("Error during DataFrame sorting: ", e)

DataFrame sorted successfully.
In [20]:
# Plot the importances
try:
    plt.figure(figsize=(10, 8))
    sns.barplot(x='importance', y='feature', data=importances_df, color='b')
    plt.title('Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()
    print("Plot created successfully.")
except Exception as e:
    print("Error during plot creation: ", e)

￼

Plot created successfully.
In [21]:
# Display the feature importances in a table
print(importances_df)

                        feature  importance
17           2_Flowering_Plants    0.090109
10                        Wasps    0.080197
5                  Parasit&Pred    0.072706
11                1_Field_Crops    0.069773
4                      Predator    0.063586
2                          Both    0.061929
20                2_Wild_Plants    0.054855
15               1_SporMisCrops    0.054160
21     2_Combo_or_Miscellaneous    0.050188
13                   1_VegHerbs    0.046559
12             1_FTreesOrchards    0.045524
3                    Parasitoid    0.044434
8                     Lacewings    0.044391
0                        Nectar    0.043262
1                        Pollen    0.034383
6                       Beetles    0.031417
7                    Hoverflies    0.029534
18                2_Fruit_Trees    0.025252
16  2_Cover_or_Cultivated_Crops    0.023134
19             2_Pasture_Forage    0.014514
9                         Mites    0.010294
14                1_VineBerries    0.009799
In [23]:
import matplotlib.pyplot as plt
import seaborn as sns

# Create a dictionary mapping each feature to its group
groups = {
    'Group 1': ['Nectar', 'Pollen', 'Both'],
    'Group 2': ['Beetles', 'Hoverflies', 'Lacewings', 'Mites', 'Wasps'],
    'Group 3': ['1_Field_Crops', '1_FTreesOrchards', '1_VegHerbs', '1_VineBerries',
       '1_SporMisCrops'],
    'Group 4': ['2_Cover_or_Cultivated_Crops', '2_Flowering_Plants',
       '2_Fruit_Trees', '2_Pasture_Forage', '2_Wild_Plants',
       '2_Combo_or_Miscellaneous'],
    'Group 5': ['Parasitoid', 'Predator', 'Parasit&Pred']
}

# Define colors for each group
colors = ['orange', 'g', 'r', 'c', 'm','gray']

# Create a barplot for each group
for i, (group, features) in enumerate(groups.items()):
    group_df = importances_df[importances_df['feature'].isin(features)]
    plt.figure(figsize=(10, 8))
    sns.barplot(x='importance', y='feature', data=group_df, color=colors[i])
    plt.title(f'Feature Importance for {group}')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.xlim(0, 0.1)  # Set the same x-axis limits for all plots
    plt.show()

￼

￼

￼

￼

￼
In [24]:
interaction_dfs = []

for i in range(len(features)):
    for j in range(i+1, len(features)):
        interaction_df = pd.DataFrame(df[features[i]] * df[features[j]], 
                                       columns=[features[i] + '_' + features[j]])
        interaction_dfs.append(interaction_df)

# Concatenate all interaction terms with original DataFrame
df_with_interactions = pd.concat([df] + interaction_dfs, axis=1)
In [25]:
# Get summary statistics for each interaction term
summary_statistics = df_with_interactions.describe()

# Display the summary statistics
print(summary_statistics)

        DVSuccess      Nectar      Pollen        Both  Parasitoid    Predator  \
count  156.000000  156.000000  156.000000  156.000000  156.000000  156.000000   
mean    58.653846    0.557692    0.108974    0.275641    0.307692    0.224359   
std     16.480939    0.498260    0.312611    0.448276    0.463025    0.418503   
min     25.000000    0.000000    0.000000    0.000000    0.000000    0.000000   
25%     50.000000    0.000000    0.000000    0.000000    0.000000    0.000000   
50%     59.000000    1.000000    0.000000    0.000000    0.000000    0.000000   
75%     67.000000    1.000000    0.000000    1.000000    1.000000    0.000000   
max    100.000000    1.000000    1.000000    1.000000    1.000000    1.000000   

       Parasit&Pred     Beetles  Hoverflies   Lacewings  ...  1_SporMisCrops  \
count    156.000000  156.000000  156.000000  156.000000  ...      156.000000   
mean       0.384615    0.102564    0.044872    0.064103  ...        0.160256   
std        0.488071    0.304366    0.207689    0.245724  ...        0.368025   
min        0.000000    0.000000    0.000000    0.000000  ...        0.000000   
25%        0.000000    0.000000    0.000000    0.000000  ...        0.000000   
50%        0.000000    0.000000    0.000000    0.000000  ...        0.000000   
75%        1.000000    0.000000    0.000000    0.000000  ...        0.000000   
max        1.000000    1.000000    1.000000    1.000000  ...        1.000000   

       2_Cover_or_Cultivated_Crops  2_Flowering_Plants  2_Fruit_Trees  \
count                   156.000000          156.000000     156.000000   
mean                      0.044872            0.371795       0.044872   
std                       0.207689            0.484840       0.207689   
min                       0.000000            0.000000       0.000000   
25%                       0.000000            0.000000       0.000000   
50%                       0.000000            0.000000       0.000000   
75%                       0.000000            1.000000       0.000000   
max                       1.000000            1.000000       1.000000   

       2_Pasture_Forage  2_Wild_Plants  2_Combo_or_Miscellaneous  \
count        156.000000     156.000000                156.000000   
mean           0.019231       0.064103                  0.115385   
std            0.137777       0.245724                  0.320514   
min            0.000000       0.000000                  0.000000   
25%            0.000000       0.000000                  0.000000   
50%            0.000000       0.000000                  0.000000   
75%            0.000000       0.000000                  0.000000   
max            1.000000       1.000000                  1.000000   

       Parasitoid_Predator  Parasitoid_Parasit&Pred  Predator_Parasit&Pred  
count                156.0                    156.0                  156.0  
mean                   0.0                      0.0                    0.0  
std                    0.0                      0.0                    0.0  
min                    0.0                      0.0                    0.0  
25%                    0.0                      0.0                    0.0  
50%                    0.0                      0.0                    0.0  
75%                    0.0                      0.0                    0.0  
max                    0.0                      0.0                    0.0  

[8 rows x 26 columns]
